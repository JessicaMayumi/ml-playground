{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d03448a",
   "metadata": {},
   "source": [
    "## üìÖ Dia 3 ‚Äì Classifica√ß√£o de Texto com Embedding + Dense\n",
    "\n",
    "### üéØ Objetivo:\n",
    "Aprender a tratar texto como entrada para redes neurais e aplicar em um caso real (sentimentos).\n",
    "\n",
    "### üìö Teoria:\n",
    "- Pr√©-processamento de texto: tokeniza√ß√£o, padding\n",
    "- Embeddings: transformar palavras em vetores\n",
    "- Arquitetura comum: Embedding ‚Üí Flatten ‚Üí Dense\n",
    "\n",
    "### üõ† Pr√°tica:\n",
    "- Dataset: IMDb (cr√≠tico de cinema ‚Äì positivo/negativo)\n",
    "- Usar Tokenizer, pad_sequences\n",
    "- Criar rede com:\n",
    "    - Embedding(input_dim, output_dim)\n",
    "    - Dense final com ativa√ß√£o sigmoid\n",
    "    - Avaliar o modelo com precis√£o e matriz de confus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8076f91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f48ed",
   "metadata": {},
   "source": [
    "# **Tokeniza√ß√£o**\n",
    "Transformar palavras em n√∫meros, para que o modelo consiga compreender melhor.\n",
    "\n",
    "### Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f115dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  [[1, 2, 3, 4], [1, 5, 2, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "frases = [\"Eu gosto de gatos\", \"Eu n√£o gosto de tigres\"]\n",
    "\n",
    "#Simulando um vocabul√°rio\n",
    "vocabulario = { \"Eu\" : 1,\n",
    "                \"gosto\" : 2,\n",
    "                \"de\" : 3, \n",
    "                \"gatos\" : 4,\n",
    "                \"n√£o\"   : 5,\n",
    "                \"tigres\" : 6\n",
    "                }\n",
    "\n",
    "#Tokenizando cada frase em IDs\n",
    "tokens = []\n",
    "for frase in frases:\n",
    "    token_frase = []\n",
    "    for palavra in frase.split():\n",
    "        token_frase.append(vocabulario[palavra])\n",
    "    tokens.append(token_frase)\n",
    "\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15aa220",
   "metadata": {},
   "source": [
    "---\n",
    "# **Padding**\n",
    "Preenche espa√ßos vazios com 0 para que as listas de n√∫meros formados por cada frase fique do mesmo tamanho, pois os modelos esperam sempre entradas com o mesmo tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5328d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4192b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 0],\n",
      "        [1, 5, 2, 3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Convertendo listas de tokens em tensores individuais\n",
    "tensor_tokens = []\n",
    "for token in tokens:\n",
    "    tensor_tokens.append(torch.tensor(token))\n",
    "\n",
    "padded_sequences = pad_sequence(tensor_tokens, batch_first=True)\n",
    "\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ba1c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Embeddings**\n",
    "Representa√ß√µes vetoriais de palavras ou tokens, com o objetivo de transformar essas strings ou tokens em vetores cont√≠nuos em um espa√ßo de alta dimens√£o com propriedades que capturam o significado e a semelhan√ßa entre as palavras.\n",
    "\n",
    "A camada ``nn.Embedding`` vai mapear esses √≠ndices(do vocabulario) para vetores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2a767",
   "metadata": {},
   "source": [
    "### **Exemplo:** \n",
    "\n",
    "Passo a passo:\n",
    "- Criar um vocabul√°rio (um dicion√°rio de palavras e seus √≠ndices).\n",
    "- Criar a camada de Embedding com a classe nn.Embedding.\n",
    "- Converter as palavras para √≠ndices.\n",
    "- Passar os √≠ndices pela camada de embedding para obter os vetores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d892d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#vocabul√°rio j√° definido anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9bdd2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "camada_embedding = nn.Embedding(num_embeddings=len(vocabulario)+1, embedding_dim=3) # (5 palavras no vocabul√°rio, com vetores de tamanho 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "376f3021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4245,  0.3057, -0.7360],\n",
      "        [-0.8371, -0.9224,  1.8113],\n",
      "        [ 0.1606,  0.3672,  0.1754],\n",
      "        [ 1.3852,  1.3835, -1.2024],\n",
      "        [-0.2234,  1.7174,  0.3189]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[-0.4245,  0.3057, -0.7360],\n",
      "        [ 0.7078, -1.0759,  0.5357],\n",
      "        [-0.8371, -0.9224,  1.8113],\n",
      "        [ 0.1606,  0.3672,  0.1754],\n",
      "        [ 1.1754,  0.5612, -0.4527]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for seq in padded_sequences:\n",
    "    print(camada_embedding(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f711f80e",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### **Plotando em 3D para verificar as semelhan√ßas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2881649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b971ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4396, -0.7581,  1.0783],\n",
      "        [ 0.8008,  1.6806,  0.3559]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[-0.6866,  0.6105,  1.3347],\n",
      "        [-0.2316,  0.0418, -0.2516]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[-0.6866,  0.6105,  1.3347],\n",
      "        [ 0.8599, -0.3097, -0.3957]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "animais = [\"gato\", \"cachorro\", \"tigre\", \"le√£o\", \"pratos\"] #pratos para verificar o qu√£o longe fica\n",
    "\n",
    "#Tokeniza√ß√£o\n",
    "tokens_animais = {  \"gato\" : 1,\n",
    "                    \"cachorro\" : 2,\n",
    "                    \"tigre\" : 3,\n",
    "                    \"le√£o\" : 4,\n",
    "                    \"pratos\" : 5\n",
    "                    }\n",
    "\n",
    "sequencias = [[\"gato\", \"cachorro\"], [\"tigre\", \"le√£o\"], [\"tigre\", \"pratos\"]] #como se fossem frases, para ter um contexto onde eles estariam juntos\n",
    "\n",
    "tokens_map = []\n",
    "for sequencia in sequencias:\n",
    "    tokens_seq = []\n",
    "    for palavra in sequencia:\n",
    "        tokens_seq.append(tokens_animais[palavra])\n",
    "    tokens_map.append(tokens_seq) #gato e cachorro = [1, 2] | tigre e leao = [3,4] | tigre e prato = [3,5]\n",
    "\n",
    "#transformando as listas de tokens em tensores\n",
    "tensor_tokens_animais = []\n",
    "for token in tokens_map: #tokens_map pois √© onde eles j√° est√£o separados por contexto\n",
    "    tensor_tokens_animais.append(torch.tensor(token, dtype=torch.long))\n",
    "\n",
    "#Padding\n",
    "padded_sequences_animais = pad_sequence(tensor_tokens_animais, batch_first=True)\n",
    "\n",
    "#Embbedding\n",
    "embeddings  = nn.Embedding(num_embeddings=len(tokens_animais)+1, embedding_dim=3)\n",
    "\n",
    "embeddings_list = []\n",
    "for seq in padded_sequences_animais:\n",
    "    embeddings_list.append(embeddings(seq))\n",
    "    print(embeddings(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "15417fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor = torch.cat(embeddings_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bed2e",
   "metadata": {},
   "source": [
    "### **Criando o Gr√°fico 3D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Acessando as colunas de embeddings para o gr√°fico 3D\n",
    "ax.scatter(embeddings_tensor[:, 0].detach().numpy(), \n",
    "           embeddings_tensor[:, 1].detach().numpy(), \n",
    "           embeddings_tensor[:, 2].detach().numpy())\n",
    "\n",
    "# Adicionando texto ao gr√°fico para cada ponto\n",
    "for i in range(len(animais)):\n",
    "    x = embeddings_tensor[i, 0].item()\n",
    "    y = embeddings_tensor[i, 1].item()\n",
    "    z = embeddings_tensor[i, 2].item()\n",
    "    ax.text(x, y, z, animais[i])\n",
    "\n",
    "plt.title(\"Visualiza√ß√£o 3D dos Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e275e",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# **Arquiteturas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d806dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Pr√°tica**\n",
    "\n",
    "- Dataset: IMDb (cr√≠tico de cinema ‚Äì positivo/negativo)\n",
    "- Usar Tokenizer, pad_sequences\n",
    "- Criar rede com:\n",
    "    - Embedding(input_dim, output_dim)\n",
    "    - Dense final com ativa√ß√£o sigmoid\n",
    "    - Avaliar o modelo com precis√£o e matriz de confus√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712c574",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
